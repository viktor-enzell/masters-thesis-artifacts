{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from kenlm import Model as KenlmModel\n",
    "from pyctcdecode.language_model import load_unigram_set_from_arpa, _prepare_unigram_set\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import remove_unwanted_chars_and_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "dataset_name = 'gw_14_socialmedia_NER_RANDOM'\n",
    "target_lang = 'sv'\n",
    "text_file_name = f'../text_data/{dataset_name}.txt'\n",
    "path_name = f'../language_models/{n}gram_{dataset_name}'\n",
    "lm_name = 'ngram'\n",
    "if not os.path.exists(path_name):\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "lm_name_arpa = f'{path_name}/{lm_name}.arpa'\n",
    "lm_name_correct = f'{path_name}/{lm_name}_correct.arpa'\n",
    "lm_name_bin = f'{path_name}/{lm_name}.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files='../korp/gigaword-2014-socialmedia.txt')['train']\n",
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(item):\n",
    "    item['text'] = remove_unwanted_chars_and_uppercase(item['text'])\n",
    "    return item\n",
    "\n",
    "dataset = dataset.map(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub('swedish_culturomics_gigaword_corpus_2010_to_2015_preprocessed', split='train', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a text file\n",
    "\n",
    "with open(text_file_name, 'w') as file:\n",
    "    file.write('\\n'.join(dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an n-gram model from .txt file\n",
    "n = n\n",
    "text_file_name = text_file_name\n",
    "lm_name_arpa = lm_name_arpa\n",
    "\n",
    "!../kenlm/build/bin/lmplz -o {n} <{text_file_name} > {lm_name_arpa}\n",
    "\n",
    "# Inspect the model\n",
    "!head -20 {lm_name_arpa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the final </s> token to the n-gram\n",
    "\n",
    "with open(lm_name_arpa, 'r') as read_file, open(lm_name_correct, 'w') as write_file:\n",
    "    has_added_eos = False\n",
    "    for line in read_file:\n",
    "        if not has_added_eos and 'ngram 1=' in line:\n",
    "            count = line.strip().split('=')[-1]\n",
    "            write_file.write(line.replace(f'{count}', f'{int(count)+1}'))\n",
    "        elif not has_added_eos and '<s>' in line:\n",
    "            write_file.write(line)\n",
    "            write_file.write(line.replace('<s>', '</s>'))\n",
    "            has_added_eos = True\n",
    "        else:\n",
    "            write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kenlm_model = KenlmModel(lm_name_arpa)\n",
    "\n",
    "unigrams = load_unigram_set_from_arpa(lm_name_arpa)\n",
    "unigram_set = _prepare_unigram_set(unigrams, kenlm_model)\n",
    "unigrams_path = f'{path_name}/unigrams.txt'\n",
    "\n",
    "with open(unigrams_path, \"w\") as fi:\n",
    "    for unigram in sorted(unigram_set):\n",
    "        fi.write(unigram + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the n-gram to binary file to reduce the size\n",
    "lm_name_arpa = lm_name_arpa\n",
    "lm_name_bin = lm_name_bin\n",
    "\n",
    "!../kenlm/build/bin/build_binary {lm_name_arpa} {lm_name_bin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the arpa files and print the binary file to see their sizes\n",
    "lm_name_arpa = lm_name_arpa\n",
    "lm_name_correct = lm_name_correct\n",
    "\n",
    "!rm {lm_name_arpa}\n",
    "!rm {lm_name_correct}\n",
    "!tree -h language_models"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
